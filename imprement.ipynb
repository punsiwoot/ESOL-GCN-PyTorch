{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "c:\\Users\\User\\AppData\\Local\\Programs\\Python\\Python39\\lib\\site-packages\\tqdm\\auto.py:22: TqdmWarning: IProgress not found. Please update jupyter and ipywidgets. See https://ipywidgets.readthedocs.io/en/stable/user_install.html\n",
      "  from .autonotebook import tqdm as notebook_tqdm\n"
     ]
    }
   ],
   "source": [
    "#import estential tool\n",
    "import rdkit\n",
    "from rdkit import Chem\n",
    "from rdkit.Chem.Draw import IPythonConsole\n",
    "import torch_geometric\n",
    "from torch_geometric.datasets import MoleculeNet\n",
    "from torch_geometric.data import Data\n",
    "import networkx as nx\n",
    "import seaborn as sb\n",
    "import sklearn\n",
    "from sklearn.metrics import mean_squared_error\n",
    "\n",
    "#for trainning\n",
    "from torch_geometric.data import DataLoader\n",
    "import warnings\n",
    "warnings.filterwarnings(\"ignore\")\n",
    "import torch\n",
    "from torch.nn import Linear\n",
    "from torch.nn import init\n",
    "import torch.nn.functional as F\n",
    "from torch_geometric.nn import GCNConv, TopKPooling, global_mean_pool\n",
    "from torch_geometric.nn import global_mean_pool as gap, global_max_pool as gmp\n",
    "import matplotlib.pyplot as plt\n",
    "import numpy as np\n",
    "import pandas as pd\n",
    "from utill import *\n",
    "from Model.BASE_model import *\n",
    "from Model.Less_layer_model import *\n",
    "from Model.More_layer_model import *\n",
    "from Model.BASE_model_tanh import *\n",
    "from Model.Less_layer_model_tanh import *\n",
    "from Model.More_layer_model_tanh import *\n",
    "from Model.BASE_model_tanh_pyra import *\n",
    "from Model.Less_layer_model_tanh_pyra import *\n",
    "from Model.More_layer_model_tanh_pyra import *\n",
    "from Model.BASE_model_pyra import *\n",
    "from Model.Less_layer_model_pyra import *\n",
    "from Model.More_layer_model_pyra import *\n",
    "device = torch.device(\"cuda:0\" if torch.cuda.is_available() else \"cpu\")\n",
    "\n",
    "seed_value = 42\n",
    "torch.manual_seed(seed_value)\n",
    "np.random.seed(seed_value)\n",
    "# Download dataset\n",
    "\n",
    "data = MoleculeNet(root=\".\", name=\"ESOL\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Modeling"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [],
   "source": [
    "#first set\n",
    "# setup \n",
    "loss_fn = torch.nn.MSELoss()\n",
    "\n",
    "#first model\n",
    "first_model = GCN_BASE_model(data_num_features=data.num_features, embedding_size=32)\n",
    "PATH_first_model_init = \"Save_weight/init_weight/first_model.pth\"\n",
    "# first_model.init_weights()\n",
    "# torch.save(first_model.state_dict(), PATH_first_model_init)\n",
    "first_model.load_state_dict(torch.load(PATH_first_model_init))\n",
    "optimizer_first = torch.optim.Adam(first_model.parameters(), lr=0.0007)\n",
    "\n",
    "#second model\n",
    "second_model = GCN_BASE_model(data_num_features=data.num_features, embedding_size=64)\n",
    "PATH_second_model_init = \"Save_weight/init_weight/second_model.pth\"\n",
    "# second_model.init_weights()\n",
    "# torch.save(second_model.state_dict(), PATH_second_model_init)\n",
    "second_model.load_state_dict(torch.load(PATH_second_model_init))\n",
    "optimizer_second = torch.optim.Adam(second_model.parameters(), lr=0.0007)\n",
    "\n",
    "#third model\n",
    "third_model = GCN_BASE_model(data_num_features=data.num_features, embedding_size=128)\n",
    "PATH_third_model_init = \"Save_weight/init_weight/third_model.pth\"\n",
    "# third_model.init_weights()\n",
    "# torch.save(third_model.state_dict(), PATH_third_model_init)\n",
    "third_model.load_state_dict(torch.load(PATH_third_model_init))\n",
    "optimizer_third = torch.optim.Adam(third_model.parameters(), lr=0.0007)\n",
    "\n",
    "#forth model\n",
    "forth_model = GCN_More_layer_model(data_num_features=data.num_features, embedding_size=32)\n",
    "PATH_forth_model_init = \"Save_weight/init_weight/forth_model.pth\"\n",
    "# forth_model.init_weights()\n",
    "# torch.save(forth_model.state_dict(), PATH_forth_model_init)\n",
    "forth_model.load_state_dict(torch.load(PATH_forth_model_init))\n",
    "optimizer_forth = torch.optim.Adam(forth_model.parameters(), lr=0.0007)\n",
    "\n",
    "#fifth model\n",
    "fifth_model = GCN_More_layer_model(data_num_features=data.num_features, embedding_size=64)\n",
    "PATH_fifth_model_init = \"Save_weight/init_weight/fifth_model.pth\"\n",
    "# fifth_model.init_weights()\n",
    "# torch.save(fifth_model.state_dict(), PATH_fifth_model_init)\n",
    "fifth_model.load_state_dict(torch.load(PATH_fifth_model_init))\n",
    "optimizer_fifth = torch.optim.Adam(fifth_model.parameters(), lr=0.0007)\n",
    "\n",
    "#sixth model\n",
    "sixth_model = GCN_More_layer_model(data_num_features=data.num_features, embedding_size=128)\n",
    "PATH_sixth_model_init = \"Save_weight/init_weight/sixth_model.pth\"\n",
    "# sixth_model.init_weights()\n",
    "# torch.save(sixth_model.state_dict(), PATH_sixth_model_init)\n",
    "sixth_model.load_state_dict(torch.load(PATH_sixth_model_init))\n",
    "optimizer_sixth = torch.optim.Adam(sixth_model.parameters(), lr=0.0007)\n",
    "\n",
    "#seventh model\n",
    "seventh_model = GCN_Less_layer_model(data_num_features=data.num_features, embedding_size=32)\n",
    "PATH_seventh_model_init = \"Save_weight/init_weight/seventh_model.pth\"\n",
    "# seventh_model.init_weights()\n",
    "# torch.save(seventh_model.state_dict(), PATH_seventh_model_init)\n",
    "seventh_model.load_state_dict(torch.load(PATH_seventh_model_init))\n",
    "optimizer_seventh = torch.optim.Adam(seventh_model.parameters(), lr=0.0007)\n",
    "\n",
    "#eighth model\n",
    "eighth_model = GCN_Less_layer_model(data_num_features=data.num_features, embedding_size=64)\n",
    "PATH_eighth_model_init = \"Save_weight/init_weight/eighth_model.pth\"\n",
    "# eighth_model.init_weights()\n",
    "# torch.save(eighth_model.state_dict(), PATH_eighth_model_init)\n",
    "eighth_model.load_state_dict(torch.load(PATH_eighth_model_init))\n",
    "optimizer_eighth = torch.optim.Adam(eighth_model.parameters(), lr=0.0007)\n",
    "\n",
    "#ninth model\n",
    "ninth_model = GCN_Less_layer_model(data_num_features=data.num_features, embedding_size=128)\n",
    "PATH_ninth_model_init = \"Save_weight/init_weight/ninth_model.pth\"\n",
    "# ninth_model.init_weights()\n",
    "# torch.save(ninth_model.state_dict(), PATH_ninth_model_init)\n",
    "ninth_model.load_state_dict(torch.load(PATH_ninth_model_init))\n",
    "optimizer_ninth = torch.optim.Adam(ninth_model.parameters(), lr=0.0007)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "GCN_BASE_model(\n",
      "  (initial_conv): GCNConv(9, 32)\n",
      "  (conv1): GCNConv(32, 32)\n",
      "  (conv2): GCNConv(32, 32)\n",
      "  (out): Linear(in_features=64, out_features=1, bias=True)\n",
      ")\n",
      "GCN_BASE_model(\n",
      "  (initial_conv): GCNConv(9, 64)\n",
      "  (conv1): GCNConv(64, 64)\n",
      "  (conv2): GCNConv(64, 64)\n",
      "  (out): Linear(in_features=128, out_features=1, bias=True)\n",
      ")\n",
      "GCN_BASE_model(\n",
      "  (initial_conv): GCNConv(9, 128)\n",
      "  (conv1): GCNConv(128, 128)\n",
      "  (conv2): GCNConv(128, 128)\n",
      "  (out): Linear(in_features=256, out_features=1, bias=True)\n",
      ")\n",
      "GCN_More_layer_model(\n",
      "  (initial_conv): GCNConv(9, 32)\n",
      "  (conv1): GCNConv(32, 32)\n",
      "  (conv2): GCNConv(32, 32)\n",
      "  (conv3): GCNConv(32, 32)\n",
      "  (out): Linear(in_features=64, out_features=1, bias=True)\n",
      ")\n",
      "GCN_More_layer_model(\n",
      "  (initial_conv): GCNConv(9, 64)\n",
      "  (conv1): GCNConv(64, 64)\n",
      "  (conv2): GCNConv(64, 64)\n",
      "  (conv3): GCNConv(64, 64)\n",
      "  (out): Linear(in_features=128, out_features=1, bias=True)\n",
      ")\n",
      "GCN_More_layer_model(\n",
      "  (initial_conv): GCNConv(9, 128)\n",
      "  (conv1): GCNConv(128, 128)\n",
      "  (conv2): GCNConv(128, 128)\n",
      "  (conv3): GCNConv(128, 128)\n",
      "  (out): Linear(in_features=256, out_features=1, bias=True)\n",
      ")\n",
      "GCN_Less_layer_model(\n",
      "  (initial_conv): GCNConv(9, 32)\n",
      "  (conv1): GCNConv(32, 32)\n",
      "  (out): Linear(in_features=64, out_features=1, bias=True)\n",
      ")\n",
      "GCN_Less_layer_model(\n",
      "  (initial_conv): GCNConv(9, 64)\n",
      "  (conv1): GCNConv(64, 64)\n",
      "  (out): Linear(in_features=128, out_features=1, bias=True)\n",
      ")\n",
      "GCN_Less_layer_model(\n",
      "  (initial_conv): GCNConv(9, 128)\n",
      "  (conv1): GCNConv(128, 128)\n",
      "  (out): Linear(in_features=256, out_features=1, bias=True)\n",
      ")\n"
     ]
    }
   ],
   "source": [
    "print(first_model)\n",
    "print(second_model)\n",
    "print(third_model)\n",
    "print(forth_model)\n",
    "print(fifth_model)\n",
    "print(sixth_model)\n",
    "print(seventh_model)\n",
    "print(eighth_model)\n",
    "print(ninth_model)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [],
   "source": [
    "#second set\n",
    "#first model\n",
    "m11 = GCN_BASE_model_tanh(data_num_features=data.num_features, embedding_size=32)\n",
    "PATH_m11_init = \"Save_weight/init_weight/m11.pth\"\n",
    "# m11.init_weights()\n",
    "# torch.save(m11.state_dict(), PATH_m11_init)\n",
    "m11.load_state_dict(torch.load(PATH_m11_init))\n",
    "optimizer_m11 = torch.optim.Adam(m11.parameters(), lr=0.0007)\n",
    "\n",
    "#second model\n",
    "m12 = GCN_BASE_model_tanh(data_num_features=data.num_features, embedding_size=64)\n",
    "PATH_m12_init = \"Save_weight/init_weight/m12.pth\"\n",
    "# m12.init_weights()\n",
    "# torch.save(m12.state_dict(), PATH_m12_init)\n",
    "m12.load_state_dict(torch.load(PATH_m12_init))\n",
    "optimizer_m12 = torch.optim.Adam(m12.parameters(), lr=0.0007)\n",
    "\n",
    "#third model\n",
    "m13 = GCN_BASE_model_tanh(data_num_features=data.num_features, embedding_size=128)\n",
    "PATH_m13_init = \"Save_weight/init_weight/m13.pth\"\n",
    "# m13.init_weights()\n",
    "# torch.save(m13.state_dict(), PATH_m13_init)\n",
    "m13.load_state_dict(torch.load(PATH_m13_init))\n",
    "optimizer_m13 = torch.optim.Adam(m13.parameters(), lr=0.0007)\n",
    "\n",
    "#forth model\n",
    "m14 = GCN_More_layer_model_tanh(data_num_features=data.num_features, embedding_size=32)\n",
    "PATH_m14_init = \"Save_weight/init_weight/m14.pth\"\n",
    "# m14.init_weights()\n",
    "# torch.save(m14.state_dict(), PATH_m14_init)\n",
    "m14.load_state_dict(torch.load(PATH_m14_init))\n",
    "optimizer_m14= torch.optim.Adam(m14.parameters(), lr=0.0007)\n",
    "\n",
    "#fifth model\n",
    "m15 = GCN_More_layer_model_tanh(data_num_features=data.num_features, embedding_size=64)\n",
    "PATH_m15_init = \"Save_weight/init_weight/m15.pth\"\n",
    "# m15.init_weights()\n",
    "# torch.save(m15.state_dict(), PATH_m15_init)\n",
    "m15.load_state_dict(torch.load(PATH_m15_init))\n",
    "optimizer_m15 = torch.optim.Adam(m15.parameters(), lr=0.0007)\n",
    "\n",
    "#sixth model\n",
    "m16 = GCN_More_layer_model_tanh(data_num_features=data.num_features, embedding_size=128)\n",
    "PATH_m16_init = \"Save_weight/init_weight/m16.pth\"\n",
    "# m16.init_weights()\n",
    "# torch.save(m16.state_dict(), PATH_m16_init)\n",
    "m16.load_state_dict(torch.load(PATH_m16_init))\n",
    "optimizer_m16 = torch.optim.Adam(m16.parameters(), lr=0.0007)\n",
    "\n",
    "#seventh model\n",
    "m17 = GCN_Less_layer_model_tanh(data_num_features=data.num_features, embedding_size=32)\n",
    "PATH_m17_init = \"Save_weight/init_weight/m17.pth\"\n",
    "# m17.init_weights()\n",
    "# torch.save(m17.state_dict(), PATH_m17_init)\n",
    "m17.load_state_dict(torch.load(PATH_m17_init))\n",
    "optimizer_m17 = torch.optim.Adam(m17.parameters(), lr=0.0007)\n",
    "\n",
    "#eighth model\n",
    "m18 = GCN_Less_layer_model_tanh(data_num_features=data.num_features, embedding_size=64)\n",
    "PATH_m18_init = \"Save_weight/init_weight/m18.pth\"\n",
    "# m18.init_weights()\n",
    "# torch.save(m18.state_dict(), PATH_m18_init)\n",
    "m18.load_state_dict(torch.load(PATH_m18_init))\n",
    "optimizer_m18 = torch.optim.Adam(m18.parameters(), lr=0.0007)\n",
    "\n",
    "#ninth model\n",
    "m19 = GCN_Less_layer_model_tanh(data_num_features=data.num_features, embedding_size=128)\n",
    "PATH_m19_init = \"Save_weight/init_weight/m19.pth\"\n",
    "# m19.init_weights()\n",
    "# torch.save(m19.state_dict(), PATH_m19_init)\n",
    "m19.load_state_dict(torch.load(PATH_m19_init))\n",
    "optimizer_m19 = torch.optim.Adam(m19.parameters(), lr=0.0007)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "GCN_BASE_model_tanh(\n",
      "  (initial_conv): GCNConv(9, 32)\n",
      "  (conv1): GCNConv(32, 32)\n",
      "  (conv2): GCNConv(32, 32)\n",
      "  (out): Linear(in_features=64, out_features=1, bias=True)\n",
      ")\n",
      "GCN_BASE_model_tanh(\n",
      "  (initial_conv): GCNConv(9, 64)\n",
      "  (conv1): GCNConv(64, 64)\n",
      "  (conv2): GCNConv(64, 64)\n",
      "  (out): Linear(in_features=128, out_features=1, bias=True)\n",
      ")\n",
      "GCN_BASE_model_tanh(\n",
      "  (initial_conv): GCNConv(9, 128)\n",
      "  (conv1): GCNConv(128, 128)\n",
      "  (conv2): GCNConv(128, 128)\n",
      "  (out): Linear(in_features=256, out_features=1, bias=True)\n",
      ")\n",
      "GCN_More_layer_model_tanh(\n",
      "  (initial_conv): GCNConv(9, 32)\n",
      "  (conv1): GCNConv(32, 32)\n",
      "  (conv2): GCNConv(32, 32)\n",
      "  (conv3): GCNConv(32, 32)\n",
      "  (out): Linear(in_features=64, out_features=1, bias=True)\n",
      ")\n",
      "GCN_More_layer_model_tanh(\n",
      "  (initial_conv): GCNConv(9, 64)\n",
      "  (conv1): GCNConv(64, 64)\n",
      "  (conv2): GCNConv(64, 64)\n",
      "  (conv3): GCNConv(64, 64)\n",
      "  (out): Linear(in_features=128, out_features=1, bias=True)\n",
      ")\n",
      "GCN_More_layer_model_tanh(\n",
      "  (initial_conv): GCNConv(9, 128)\n",
      "  (conv1): GCNConv(128, 128)\n",
      "  (conv2): GCNConv(128, 128)\n",
      "  (conv3): GCNConv(128, 128)\n",
      "  (out): Linear(in_features=256, out_features=1, bias=True)\n",
      ")\n",
      "GCN_Less_layer_model_tanh(\n",
      "  (initial_conv): GCNConv(9, 32)\n",
      "  (conv1): GCNConv(32, 32)\n",
      "  (out): Linear(in_features=64, out_features=1, bias=True)\n",
      ")\n",
      "GCN_Less_layer_model_tanh(\n",
      "  (initial_conv): GCNConv(9, 64)\n",
      "  (conv1): GCNConv(64, 64)\n",
      "  (out): Linear(in_features=128, out_features=1, bias=True)\n",
      ")\n",
      "GCN_Less_layer_model_tanh(\n",
      "  (initial_conv): GCNConv(9, 128)\n",
      "  (conv1): GCNConv(128, 128)\n",
      "  (out): Linear(in_features=256, out_features=1, bias=True)\n",
      ")\n"
     ]
    }
   ],
   "source": [
    "print(m11)\n",
    "print(m12)\n",
    "print(m13)\n",
    "print(m14)\n",
    "print(m15)\n",
    "print(m16)\n",
    "print(m17)\n",
    "print(m18)\n",
    "print(m19)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {},
   "outputs": [],
   "source": [
    "#third set\n",
    "#first model\n",
    "m21 = GCN_BASE_model_pyra(data_num_features=data.num_features, embedding_size=32)\n",
    "PATH_m21_init = \"Save_weight/init_weight/m21.pth\"\n",
    "# m21.init_weights()\n",
    "# torch.save(m21.state_dict(), PATH_m21_init)\n",
    "m21.load_state_dict(torch.load(PATH_m21_init))\n",
    "optimizer_m21 = torch.optim.Adam(m21.parameters(), lr=0.0007)\n",
    "\n",
    "#second model\n",
    "m22 = GCN_BASE_model_pyra(data_num_features=data.num_features, embedding_size=64)\n",
    "PATH_m22_init = \"Save_weight/init_weight/m22.pth\"\n",
    "# m22.init_weights()\n",
    "# torch.save(m22.state_dict(), PATH_m22_init)\n",
    "m22.load_state_dict(torch.load(PATH_m22_init))\n",
    "optimizer_m22 = torch.optim.Adam(m22.parameters(), lr=0.0007)\n",
    "\n",
    "#third model\n",
    "m23 = GCN_BASE_model_pyra(data_num_features=data.num_features, embedding_size=128)\n",
    "PATH_m23_init = \"Save_weight/init_weight/m23.pth\"\n",
    "# m23.init_weights()\n",
    "# torch.save(m23.state_dict(), PATH_m23_init)\n",
    "m23.load_state_dict(torch.load(PATH_m23_init))\n",
    "optimizer_m23 = torch.optim.Adam(m23.parameters(), lr=0.0007)\n",
    "\n",
    "#forth model\n",
    "m24 = GCN_More_layer_model_pyra(data_num_features=data.num_features, embedding_size=32)\n",
    "PATH_m24_init = \"Save_weight/init_weight/m24.pth\"\n",
    "# m24.init_weights()\n",
    "# torch.save(m24.state_dict(), PATH_m24_init)\n",
    "m24.load_state_dict(torch.load(PATH_m24_init))\n",
    "optimizer_m24= torch.optim.Adam(m24.parameters(), lr=0.0007)\n",
    "\n",
    "#fifth model\n",
    "m25 = GCN_More_layer_model_pyra(data_num_features=data.num_features, embedding_size=64)\n",
    "PATH_m25_init = \"Save_weight/init_weight/m25.pth\"\n",
    "# m25.init_weights()\n",
    "# torch.save(m25.state_dict(), PATH_m25_init)\n",
    "m25.load_state_dict(torch.load(PATH_m25_init))\n",
    "optimizer_m25 = torch.optim.Adam(m25.parameters(), lr=0.0007)\n",
    "\n",
    "#sixth model\n",
    "m26 = GCN_More_layer_model_pyra(data_num_features=data.num_features, embedding_size=128)\n",
    "PATH_m26_init = \"Save_weight/init_weight/m26.pth\"\n",
    "# m26.init_weights()\n",
    "# torch.save(m26.state_dict(), PATH_m26_init)\n",
    "m26.load_state_dict(torch.load(PATH_m26_init))\n",
    "optimizer_m26 = torch.optim.Adam(m26.parameters(), lr=0.0007)\n",
    "\n",
    "#seventh model\n",
    "m27 = GCN_Less_layer_model_pyra(data_num_features=data.num_features, embedding_size=32)\n",
    "PATH_m27_init = \"Save_weight/init_weight/m27.pth\"\n",
    "# m27.init_weights()\n",
    "# torch.save(m27.state_dict(), PATH_m27_init)\n",
    "m27.load_state_dict(torch.load(PATH_m27_init))\n",
    "optimizer_m27 = torch.optim.Adam(m27.parameters(), lr=0.0007)\n",
    "\n",
    "#eighth model\n",
    "m28 = GCN_Less_layer_model_pyra(data_num_features=data.num_features, embedding_size=64)\n",
    "PATH_m28_init = \"Save_weight/init_weight/m28.pth\"\n",
    "# m28.init_weights()\n",
    "# torch.save(m28.state_dict(), PATH_m28_init)\n",
    "m28.load_state_dict(torch.load(PATH_m28_init))\n",
    "optimizer_m28 = torch.optim.Adam(m28.parameters(), lr=0.0007)\n",
    "\n",
    "#ninth model\n",
    "m29 = GCN_Less_layer_model_pyra(data_num_features=data.num_features, embedding_size=128)\n",
    "PATH_m29_init = \"Save_weight/init_weight/m29.pth\"\n",
    "# m29.init_weights()\n",
    "# torch.save(m29.state_dict(), PATH_m29_init)\n",
    "m29.load_state_dict(torch.load(PATH_m29_init))\n",
    "optimizer_m29 = torch.optim.Adam(m29.parameters(), lr=0.0007)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "GCN_BASE_model_pyra(\n",
      "  (initial_conv): GCNConv(9, 32)\n",
      "  (conv1): GCNConv(32, 16)\n",
      "  (conv2): GCNConv(16, 8)\n",
      "  (out): Linear(in_features=16, out_features=1, bias=True)\n",
      ")\n",
      "GCN_BASE_model_pyra(\n",
      "  (initial_conv): GCNConv(9, 64)\n",
      "  (conv1): GCNConv(64, 32)\n",
      "  (conv2): GCNConv(32, 16)\n",
      "  (out): Linear(in_features=32, out_features=1, bias=True)\n",
      ")\n",
      "GCN_BASE_model_pyra(\n",
      "  (initial_conv): GCNConv(9, 128)\n",
      "  (conv1): GCNConv(128, 64)\n",
      "  (conv2): GCNConv(64, 32)\n",
      "  (out): Linear(in_features=64, out_features=1, bias=True)\n",
      ")\n",
      "GCN_More_layer_model_pyra(\n",
      "  (initial_conv): GCNConv(9, 32)\n",
      "  (conv1): GCNConv(32, 16)\n",
      "  (conv2): GCNConv(16, 8)\n",
      "  (conv3): GCNConv(8, 4)\n",
      "  (out): Linear(in_features=8, out_features=1, bias=True)\n",
      ")\n",
      "GCN_More_layer_model_pyra(\n",
      "  (initial_conv): GCNConv(9, 64)\n",
      "  (conv1): GCNConv(64, 32)\n",
      "  (conv2): GCNConv(32, 16)\n",
      "  (conv3): GCNConv(16, 8)\n",
      "  (out): Linear(in_features=16, out_features=1, bias=True)\n",
      ")\n",
      "GCN_More_layer_model_pyra(\n",
      "  (initial_conv): GCNConv(9, 128)\n",
      "  (conv1): GCNConv(128, 64)\n",
      "  (conv2): GCNConv(64, 32)\n",
      "  (conv3): GCNConv(32, 16)\n",
      "  (out): Linear(in_features=32, out_features=1, bias=True)\n",
      ")\n",
      "GCN_Less_layer_model_pyra(\n",
      "  (initial_conv): GCNConv(9, 32)\n",
      "  (conv1): GCNConv(32, 16)\n",
      "  (out): Linear(in_features=32, out_features=1, bias=True)\n",
      ")\n",
      "GCN_Less_layer_model_pyra(\n",
      "  (initial_conv): GCNConv(9, 64)\n",
      "  (conv1): GCNConv(64, 32)\n",
      "  (out): Linear(in_features=64, out_features=1, bias=True)\n",
      ")\n",
      "GCN_Less_layer_model_pyra(\n",
      "  (initial_conv): GCNConv(9, 128)\n",
      "  (conv1): GCNConv(128, 64)\n",
      "  (out): Linear(in_features=128, out_features=1, bias=True)\n",
      ")\n"
     ]
    }
   ],
   "source": [
    "print(m21)\n",
    "print(m22)\n",
    "print(m23)\n",
    "print(m24)\n",
    "print(m25)\n",
    "print(m26)\n",
    "print(m27)\n",
    "print(m28)\n",
    "print(m29)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 17,
   "metadata": {},
   "outputs": [],
   "source": [
    "#forth set\n",
    "#first model\n",
    "m31 = GCN_BASE_model_tanh_pyra(data_num_features=data.num_features, embedding_size=32)\n",
    "PATH_m31_init = \"Save_weight/init_weight/m31.pth\"\n",
    "# m31.init_weights()\n",
    "# torch.save(m31.state_dict(), PATH_m31_init)\n",
    "m31.load_state_dict(torch.load(PATH_m31_init))\n",
    "optimizer_m31 = torch.optim.Adam(m31.parameters(), lr=0.0007)\n",
    "\n",
    "#second model\n",
    "m32 = GCN_BASE_model_tanh_pyra(data_num_features=data.num_features, embedding_size=64)\n",
    "PATH_m32_init = \"Save_weight/init_weight/m32.pth\"\n",
    "# m32.init_weights()\n",
    "# torch.save(m32.state_dict(), PATH_m32_init)\n",
    "m32.load_state_dict(torch.load(PATH_m32_init))\n",
    "optimizer_m32 = torch.optim.Adam(m32.parameters(), lr=0.0007)\n",
    "\n",
    "#third model\n",
    "m33 = GCN_BASE_model_tanh_pyra(data_num_features=data.num_features, embedding_size=128)\n",
    "PATH_m33_init = \"Save_weight/init_weight/m33.pth\"\n",
    "# m33.init_weights()\n",
    "# torch.save(m33.state_dict(), PATH_m33_init)\n",
    "m33.load_state_dict(torch.load(PATH_m33_init))\n",
    "optimizer_m33 = torch.optim.Adam(m33.parameters(), lr=0.0007)\n",
    "\n",
    "#forth model\n",
    "m34 = GCN_More_layer_model_tanh_pyra(data_num_features=data.num_features, embedding_size=32)\n",
    "PATH_m34_init = \"Save_weight/init_weight/m34.pth\"\n",
    "# m34.init_weights()\n",
    "# torch.save(m34.state_dict(), PATH_m34_init)\n",
    "m34.load_state_dict(torch.load(PATH_m34_init))\n",
    "optimizer_m34= torch.optim.Adam(m34.parameters(), lr=0.0007)\n",
    "\n",
    "#fifth model\n",
    "m35 = GCN_More_layer_model_tanh_pyra(data_num_features=data.num_features, embedding_size=64)\n",
    "PATH_m35_init = \"Save_weight/init_weight/m35.pth\"\n",
    "# m35.init_weights()\n",
    "# torch.save(m35.state_dict(), PATH_m35_init)\n",
    "m35.load_state_dict(torch.load(PATH_m35_init))\n",
    "optimizer_m35 = torch.optim.Adam(m35.parameters(), lr=0.0007)\n",
    "\n",
    "#sixth model\n",
    "m36 = GCN_More_layer_model_tanh_pyra(data_num_features=data.num_features, embedding_size=128)\n",
    "PATH_m36_init = \"Save_weight/init_weight/m36.pth\"\n",
    "# m36.init_weights()\n",
    "# torch.save(m36.state_dict(), PATH_m36_init)\n",
    "m36.load_state_dict(torch.load(PATH_m36_init))\n",
    "optimizer_m36 = torch.optim.Adam(m36.parameters(), lr=0.0007)\n",
    "\n",
    "#seventh model\n",
    "m37 = GCN_Less_layer_model_tanh_pyra(data_num_features=data.num_features, embedding_size=32)\n",
    "PATH_m37_init = \"Save_weight/init_weight/m37.pth\"\n",
    "# m37.init_weights()\n",
    "# torch.save(m37.state_dict(), PATH_m37_init)\n",
    "m37.load_state_dict(torch.load(PATH_m37_init))\n",
    "optimizer_m37 = torch.optim.Adam(m37.parameters(), lr=0.0007)\n",
    "\n",
    "#eighth model\n",
    "m38 = GCN_Less_layer_model_tanh_pyra(data_num_features=data.num_features, embedding_size=64)\n",
    "PATH_m38_init = \"Save_weight/init_weight/m38.pth\"\n",
    "# m38.init_weights()\n",
    "# torch.save(m38.state_dict(), PATH_m38_init)\n",
    "m38.load_state_dict(torch.load(PATH_m38_init))\n",
    "optimizer_m38 = torch.optim.Adam(m38.parameters(), lr=0.0007)\n",
    "\n",
    "#ninth model\n",
    "m39 = GCN_Less_layer_model_tanh_pyra(data_num_features=data.num_features, embedding_size=128)\n",
    "PATH_m39_init = \"Save_weight/init_weight/m39.pth\"\n",
    "# m39.init_weights()\n",
    "# torch.save(m39.state_dict(), PATH_m39_init)\n",
    "m39.load_state_dict(torch.load(PATH_m39_init))\n",
    "optimizer_m39 = torch.optim.Adam(m39.parameters(), lr=0.0007)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 18,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "GCN_BASE_model_tanh_pyra(\n",
      "  (initial_conv): GCNConv(9, 32)\n",
      "  (conv1): GCNConv(32, 16)\n",
      "  (conv2): GCNConv(16, 8)\n",
      "  (out): Linear(in_features=16, out_features=1, bias=True)\n",
      ")\n",
      "GCN_BASE_model_tanh_pyra(\n",
      "  (initial_conv): GCNConv(9, 64)\n",
      "  (conv1): GCNConv(64, 32)\n",
      "  (conv2): GCNConv(32, 16)\n",
      "  (out): Linear(in_features=32, out_features=1, bias=True)\n",
      ")\n",
      "GCN_BASE_model_tanh_pyra(\n",
      "  (initial_conv): GCNConv(9, 128)\n",
      "  (conv1): GCNConv(128, 64)\n",
      "  (conv2): GCNConv(64, 32)\n",
      "  (out): Linear(in_features=64, out_features=1, bias=True)\n",
      ")\n",
      "GCN_More_layer_model_tanh_pyra(\n",
      "  (initial_conv): GCNConv(9, 32)\n",
      "  (conv1): GCNConv(32, 16)\n",
      "  (conv2): GCNConv(16, 8)\n",
      "  (conv3): GCNConv(8, 4)\n",
      "  (out): Linear(in_features=8, out_features=1, bias=True)\n",
      ")\n",
      "GCN_More_layer_model_tanh_pyra(\n",
      "  (initial_conv): GCNConv(9, 64)\n",
      "  (conv1): GCNConv(64, 32)\n",
      "  (conv2): GCNConv(32, 16)\n",
      "  (conv3): GCNConv(16, 8)\n",
      "  (out): Linear(in_features=16, out_features=1, bias=True)\n",
      ")\n",
      "GCN_More_layer_model_tanh_pyra(\n",
      "  (initial_conv): GCNConv(9, 128)\n",
      "  (conv1): GCNConv(128, 64)\n",
      "  (conv2): GCNConv(64, 32)\n",
      "  (conv3): GCNConv(32, 16)\n",
      "  (out): Linear(in_features=32, out_features=1, bias=True)\n",
      ")\n",
      "GCN_Less_layer_model_tanh_pyra(\n",
      "  (initial_conv): GCNConv(9, 32)\n",
      "  (conv1): GCNConv(32, 16)\n",
      "  (out): Linear(in_features=32, out_features=1, bias=True)\n",
      ")\n",
      "GCN_Less_layer_model_tanh_pyra(\n",
      "  (initial_conv): GCNConv(9, 64)\n",
      "  (conv1): GCNConv(64, 32)\n",
      "  (out): Linear(in_features=64, out_features=1, bias=True)\n",
      ")\n",
      "GCN_Less_layer_model_tanh_pyra(\n",
      "  (initial_conv): GCNConv(9, 128)\n",
      "  (conv1): GCNConv(128, 64)\n",
      "  (out): Linear(in_features=128, out_features=1, bias=True)\n",
      ")\n"
     ]
    }
   ],
   "source": [
    "print(m31)\n",
    "print(m32)\n",
    "print(m33)\n",
    "print(m34)\n",
    "print(m35)\n",
    "print(m36)\n",
    "print(m37)\n",
    "print(m38)\n",
    "print(m39)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Setup"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [],
   "source": [
    "# setup dataset\n",
    "data_size = len(data)\n",
    "NUM_GRAPHS_PER_BATCH = 32\n",
    "training_set = DataLoader(data[:int(data_size * 0.7)],\n",
    "                    batch_size=NUM_GRAPHS_PER_BATCH, shuffle=True)\n",
    "test_set = DataLoader(data[int(data_size * 0.7):int(data_size * 0.85)],\n",
    "                         batch_size=NUM_GRAPHS_PER_BATCH, shuffle=True)\n",
    "validation_set = DataLoader(data[int(data_size * 0.85):],\n",
    "                         batch_size=NUM_GRAPHS_PER_BATCH, shuffle=True)\n",
    "\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Training"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "at epoch : 1 train_loss = 1223.36765625  valid_loss = 444.2113978068034\n",
      "at epoch : 2 train_loss = 218.0674462890625  valid_loss = 180.34984334309897\n",
      "at epoch : 3 train_loss = 140.02480194091797  valid_loss = 153.50049463907877\n",
      "at epoch : 4 train_loss = 119.71230773925781  valid_loss = 131.25205357869467\n",
      "at epoch : 5 train_loss = 106.41873245239258  valid_loss = 114.71997578938802\n"
     ]
    }
   ],
   "source": [
    "# first_model trainning\n",
    "first_model = GCN_BASE_model(data_num_features=data.num_features, embedding_size=32)\n",
    "first_model.load_state_dict(torch.load(PATH_first_model_init))\n",
    "optimizer_first = torch.optim.Adam(first_model.parameters(), lr=0.0007)\n",
    "loss_train_track_first, loss_valid_track_first, stop_at_epoch_first = train(model=first_model, optimizer=optimizer_first, loss_fn=loss_fn,\n",
    "                                                                            epochs=5, is_early_stop=False, show_result_at=1,\n",
    "                                                                            device=device, validation_set=validation_set, training_set=training_set)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 52,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "case valid better case 1\n",
      "case valid better case 2\n",
      "case valid better case 3\n",
      "case valid better case 1\n"
     ]
    },
    {
     "data": {
      "text/plain": [
       "[253.21293131510416, 219.31498962402344, 146.00571568806967]"
      ]
     },
     "execution_count": 52,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "# second_model trainning\n",
    "second_model = GCN_BASE_model(data_num_features=data.num_features, embedding_size=64)\n",
    "second_model.load_state_dict(torch.load(PATH_second_model_init))\n",
    "optimizer_second = torch.optim.Adam(second_model.parameters(), lr=0.0007)\n",
    "loss_train_track_second, loss_valid_track_second, stop_at_epoch_second = train(model=second_model, optimizer=optimizer_second, loss_fn=loss_fn,\n",
    "                                                                            epochs=5, is_early_stop=False, show_result_at=1,\n",
    "                                                                            device=device, validation_set=validation_set, training_set=training_set)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "at epoch : 1 train_loss = 14010.541247558594  valid_loss = 7101.525227864583\n",
      "at epoch : 2 train_loss = 3068.4201904296874  valid_loss = 1198.3312072753906\n",
      "at epoch : 3 train_loss = 1000.2343969726562  valid_loss = 457.67698160807294\n",
      "at epoch : 4 train_loss = 652.530283203125  valid_loss = 422.07728068033856\n",
      "at epoch : 5 train_loss = 521.9027917480469  valid_loss = 334.9480489095052\n"
     ]
    }
   ],
   "source": [
    "# third_model trainning\n",
    "third_model = GCN_BASE_model(data_num_features=data.num_features, embedding_size=128)\n",
    "third_model.load_state_dict(torch.load(PATH_third_model_init))\n",
    "optimizer_third = torch.optim.Adam(third_model.parameters(), lr=0.0007)\n",
    "loss_train_track_third, loss_valid_track_third, stop_at_epoch_third = train(model=third_model, optimizer=optimizer_third, loss_fn=loss_fn,\n",
    "                                                                            epochs=5, is_early_stop=False, show_result_at=1,\n",
    "                                                                            device=device, validation_set=validation_set, training_set=training_set)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 14,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "at epoch : 1 train_loss = 168668.0931640625  valid_loss = 14573.262451171875\n",
      "at epoch : 2 train_loss = 14536.6262109375  valid_loss = 9919.899820963541\n",
      "at epoch : 3 train_loss = 7700.442963867187  valid_loss = 7732.3857421875\n",
      "at epoch : 4 train_loss = 5078.977646484375  valid_loss = 4768.129150390625\n",
      "at epoch : 5 train_loss = 3945.779931640625  valid_loss = 4209.951985677083\n"
     ]
    }
   ],
   "source": [
    "# forth_model trainning\n",
    "forth_model = GCN_More_layer_model(data_num_features=data.num_features, embedding_size=32)\n",
    "forth_model.load_state_dict(torch.load(PATH_forth_model_init))\n",
    "optimizer_forth = torch.optim.Adam(forth_model.parameters(), lr=0.0007)\n",
    "loss_train_track_forth, loss_valid_track_forth, stop_at_epoch_forth = train(model=forth_model, optimizer=optimizer_forth, loss_fn=loss_fn,\n",
    "                                                                            epochs=5, is_early_stop=False, show_result_at=1,\n",
    "                                                                            device=device, validation_set=validation_set, training_set=training_set)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 15,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "at epoch : 1 train_loss = 290317.925  valid_loss = 80146.95833333333\n",
      "at epoch : 2 train_loss = 47168.927578125  valid_loss = 33557.8056640625\n",
      "at epoch : 3 train_loss = 26031.3353125  valid_loss = 22366.338704427082\n",
      "at epoch : 4 train_loss = 18155.06140625  valid_loss = 16714.211100260418\n",
      "at epoch : 5 train_loss = 14377.90279296875  valid_loss = 15549.471842447916\n"
     ]
    }
   ],
   "source": [
    "# fifth_model trainning\n",
    "fifth_model = GCN_More_layer_model(data_num_features=data.num_features, embedding_size=64)\n",
    "fifth_model.load_state_dict(torch.load(PATH_fifth_model_init))\n",
    "optimizer_fifth = torch.optim.Adam(fifth_model.parameters(), lr=0.0007)\n",
    "loss_train_track_fifth, loss_valid_track_fifth, stop_at_epoch_fifth = train(model=fifth_model, optimizer=optimizer_fifth, loss_fn=loss_fn,\n",
    "                                                                            epochs=5, is_early_stop=False, show_result_at=1,\n",
    "                                                                            device=device, validation_set=validation_set, training_set=training_set)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 16,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "at epoch : 1 train_loss = 653378.670625  valid_loss = 169392.13802083334\n",
      "at epoch : 2 train_loss = 79683.50765625  valid_loss = 75401.412109375\n",
      "at epoch : 3 train_loss = 46895.375390625  valid_loss = 47709.873697916664\n",
      "at epoch : 4 train_loss = 34297.579921875  valid_loss = 37001.234049479164\n",
      "at epoch : 5 train_loss = 26660.2360546875  valid_loss = 28468.814778645832\n"
     ]
    }
   ],
   "source": [
    "# sixth_model trainning\n",
    "sixth_model = GCN_More_layer_model(data_num_features=data.num_features, embedding_size=128)\n",
    "sixth_model.load_state_dict(torch.load(PATH_sixth_model_init))\n",
    "optimizer_sixth = torch.optim.Adam(sixth_model.parameters(), lr=0.0007)\n",
    "loss_train_track_sixth, loss_valid_track_sixth, stop_at_epoch_sixth = train(model=sixth_model, optimizer=optimizer_sixth, loss_fn=loss_fn,\n",
    "                                                                            epochs=5, is_early_stop=False, show_result_at=1,\n",
    "                                                                            device=device, validation_set=validation_set, training_set=training_set)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 20,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "at epoch : 1 train_loss = 457.20014053344727  valid_loss = 54.9495644569397\n",
      "at epoch : 2 train_loss = 50.36080497741699  valid_loss = 46.65918699900309\n",
      "at epoch : 3 train_loss = 35.57845058441162  valid_loss = 30.560200055440266\n",
      "at epoch : 4 train_loss = 26.049870090484617  valid_loss = 23.178395748138428\n",
      "at epoch : 5 train_loss = 19.84479419708252  valid_loss = 23.055582523345947\n"
     ]
    }
   ],
   "source": [
    "# seventh_model trainning\n",
    "seventh_model = GCN_Less_layer_model(data_num_features=data.num_features, embedding_size=32)\n",
    "seventh_model.load_state_dict(torch.load(PATH_seventh_model_init))\n",
    "optimizer_seventh = torch.optim.Adam(seventh_model.parameters(), lr=0.0007)\n",
    "loss_train_track_seventh, loss_valid_track_seventh, stop_at_epoch_seventh = train(model=seventh_model, optimizer=optimizer_seventh, loss_fn=loss_fn,\n",
    "                                                                            epochs=5, is_early_stop=False, show_result_at=1,\n",
    "                                                                            device=device, validation_set=validation_set, training_set=training_set)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 22,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "at epoch : 1 train_loss = 77.4163667678833  valid_loss = 28.690818468729656\n",
      "at epoch : 2 train_loss = 17.58393970489502  valid_loss = 13.077176411946615\n",
      "at epoch : 3 train_loss = 11.762789325714111  valid_loss = 11.498063882191977\n",
      "at epoch : 4 train_loss = 9.885744438171386  valid_loss = 10.528969685236612\n",
      "at epoch : 5 train_loss = 8.812910270690917  valid_loss = 10.373045921325684\n"
     ]
    }
   ],
   "source": [
    "# eighth_model trainning\n",
    "eighth_model = GCN_Less_layer_model(data_num_features=data.num_features, embedding_size=64)\n",
    "eighth_model.load_state_dict(torch.load(PATH_eighth_model_init))\n",
    "optimizer_eighth = torch.optim.Adam(eighth_model.parameters(), lr=0.0007)\n",
    "loss_train_track_eighth, loss_valid_track_eighth, stop_at_epoch_eighth = train(model=eighth_model, optimizer=optimizer_eighth, loss_fn=loss_fn,\n",
    "                                                                            epochs=5, is_early_stop=False, show_result_at=1,\n",
    "                                                                            device=device, validation_set=validation_set, training_set=training_set)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 23,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "at epoch : 1 train_loss = 1114.9195788574218  valid_loss = 282.3285140991211\n",
      "at epoch : 2 train_loss = 152.2808544921875  valid_loss = 57.048532803853355\n",
      "at epoch : 3 train_loss = 39.998948135375976  valid_loss = 27.512378374735516\n",
      "at epoch : 4 train_loss = 26.421061401367187  valid_loss = 25.96221923828125\n",
      "at epoch : 5 train_loss = 22.711476974487304  valid_loss = 21.40555222829183\n"
     ]
    }
   ],
   "source": [
    "# ninth_model trainning\n",
    "ninth_model = GCN_Less_layer_model(data_num_features=data.num_features, embedding_size=128)\n",
    "ninth_model.load_state_dict(torch.load(PATH_ninth_model_init))\n",
    "optimizer_ninth = torch.optim.Adam(ninth_model.parameters(), lr=0.0007)\n",
    "loss_train_track_ninth, loss_valid_track_ninth, stop_at_epoch_ninth = train(model=ninth_model, optimizer=optimizer_ninth, loss_fn=loss_fn,\n",
    "                                                                            epochs=5, is_early_stop=False, show_result_at=1,\n",
    "                                                                            device=device, validation_set=validation_set, training_set=training_set)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 53,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "0.8329761435263996"
      ]
     },
     "execution_count": 53,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "# sklearn.metrics.mean_squared_error(df['y_real'], df['y_pred'])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# # first model\n",
    "# embedding_size = 64\n",
    "# class GCN_first_model(torch.nn.Module):\n",
    "#     def __init__(self):\n",
    "#         torch.manual_seed(seed_value)\n",
    "#         # Init parent\n",
    "#         super(GCN_first_model, self).__init__()\n",
    "\n",
    "#         # define layer\n",
    "#         self.initial_conv = GCNConv(data.num_features, embedding_size)\n",
    "#         self.conv1 = GCNConv(embedding_size, embedding_size)\n",
    "#         self.conv2 = GCNConv(embedding_size, embedding_size)\n",
    "    \n",
    "#         # define linear layer\n",
    "#         self.out = Linear(embedding_size*2, 1)\n",
    "    \n",
    "#     def init_weights(self):\n",
    "#         # Use Xavier/Glorot initialization for GCNConv layers\n",
    "#         for layer in [self.initial_conv, self.conv1, self.conv2]:\n",
    "#             if isinstance(layer, GCNConv):\n",
    "#                 # layer.reset_parameters()\n",
    "#                 if isinstance(layer, GCNConv):\n",
    "#                     init.normal_(layer.lin.weight)\n",
    "#                     if layer.bias is not None:\n",
    "#                         init.zeros_(layer.bias)\n",
    "\n",
    "#         # Initialize weights for the Linear layer\n",
    "#         init.xavier_normal_(self.out.weight)\n",
    "#         if self.out.bias is not None:\n",
    "#             init.zeros_(self.out.bias)\n",
    "\n",
    "#     def forward(self, x, edge_index, batch_index):\n",
    "        \n",
    "#         # first layer\n",
    "#         hidden = self.initial_conv(x, edge_index)\n",
    "#         hidden = F.relu(hidden)\n",
    "#         # second layer\n",
    "#         hidden = self.conv1(hidden, edge_index)\n",
    "#         hidden = F.relu(hidden)\n",
    "#         # third layer\n",
    "#         hidden = self.conv2(hidden, edge_index)\n",
    "#         hidden = F.relu(hidden)\n",
    "\n",
    "#         # global pooling\n",
    "#         hidden = torch.cat([gmp(hidden, batch_index),\n",
    "#                             gap(hidden, batch_index)], dim=1)\n",
    "                            \n",
    "\n",
    "#         # apply linear layer\n",
    "#         out = self.out(hidden)\n",
    "#         return out\n",
    "    \n",
    "\n",
    "# # second model\n",
    "# embedding_size = 32\n",
    "# class GCN_second_model(torch.nn.Module):\n",
    "#     def __init__(self):\n",
    "#         torch.manual_seed(seed_value)\n",
    "#         # Init parent\n",
    "#         super(GCN_second_model, self).__init__()\n",
    "\n",
    "#         # define layer\n",
    "#         self.initial_conv = GCNConv(data.num_features, embedding_size)\n",
    "#         self.conv1 = GCNConv(embedding_size, embedding_size)\n",
    "#         self.conv2 = GCNConv(embedding_size, embedding_size)\n",
    "    \n",
    "#         # define linear layer\n",
    "#         self.out = Linear(embedding_size*2, 1)\n",
    "\n",
    "#     def init_weights(self):\n",
    "#         # Use Xavier/Glorot initialization for GCNConv layers\n",
    "#         for layer in [self.initial_conv, self.conv1, self.conv2]:\n",
    "#             if isinstance(layer, GCNConv):\n",
    "#                 # layer.reset_parameters()\n",
    "#                 if isinstance(layer, GCNConv):\n",
    "#                     init.normal_(layer.lin.weight)\n",
    "#                     if layer.bias is not None:\n",
    "#                         init.zeros_(layer.bias)\n",
    "\n",
    "#         # Initialize weights for the Linear layer\n",
    "#         init.xavier_normal_(self.out.weight)\n",
    "#         if self.out.bias is not None:\n",
    "#             init.zeros_(self.out.bias)\n",
    "\n",
    "#     def forward(self, x, edge_index, batch_index):\n",
    "        \n",
    "#         # first layer\n",
    "#         hidden = self.initial_conv(x, edge_index)\n",
    "#         hidden = F.relu(hidden)\n",
    "#         # second layer\n",
    "#         hidden = self.conv1(hidden, edge_index)\n",
    "#         hidden = F.relu(hidden)\n",
    "#         # third layer\n",
    "#         hidden = self.conv2(hidden, edge_index)\n",
    "#         hidden = F.relu(hidden)\n",
    "\n",
    "#         # global pooling\n",
    "#         hidden = torch.cat([gmp(hidden, batch_index),\n",
    "#                             gap(hidden, batch_index)], dim=1)\n",
    "\n",
    "#         # apply linear layer\n",
    "#         out = self.out(hidden)\n",
    "#         return out\n",
    "\n",
    "# # third model\n",
    "# embedding_size = 128\n",
    "# class GCN_third_model(torch.nn.Module):\n",
    "#     def __init__(self):\n",
    "#         torch.manual_seed(seed_value)\n",
    "#         # Init parent\n",
    "#         super(GCN_third_model, self).__init__()\n",
    "\n",
    "#         # define layer\n",
    "#         self.initial_conv = GCNConv(data.num_features, embedding_size)\n",
    "#         self.conv1 = GCNConv(embedding_size, embedding_size)\n",
    "#         self.conv2 = GCNConv(embedding_size, embedding_size)\n",
    "    \n",
    "#         # define linear layer\n",
    "#         self.out = Linear(embedding_size*2, 1)\n",
    "\n",
    "#     def init_weights(self):\n",
    "#         # Use Xavier/Glorot initialization for GCNConv layers\n",
    "#         for layer in [self.initial_conv, self.conv1, self.conv2]:\n",
    "#             if isinstance(layer, GCNConv):\n",
    "#                 # layer.reset_parameters()\n",
    "#                 if isinstance(layer, GCNConv):\n",
    "#                     init.normal_(layer.lin.weight)\n",
    "#                     if layer.bias is not None:\n",
    "#                         init.zeros_(layer.bias)\n",
    "\n",
    "#         # Initialize weights for the Linear layer\n",
    "#         init.xavier_normal_(self.out.weight)\n",
    "#         if self.out.bias is not None:\n",
    "#             init.zeros_(self.out.bias)\n",
    "\n",
    "#     def forward(self, x, edge_index, batch_index):\n",
    "        \n",
    "#         # first layer\n",
    "#         hidden = self.initial_conv(x, edge_index)\n",
    "#         hidden = F.relu(hidden)\n",
    "#         # second layer\n",
    "#         hidden = self.conv1(hidden, edge_index)\n",
    "#         hidden = F.relu(hidden)\n",
    "#         # third layer\n",
    "#         hidden = self.conv2(hidden, edge_index)\n",
    "#         hidden = F.relu(hidden)\n",
    "\n",
    "#         # global pooling\n",
    "#         hidden = torch.cat([gmp(hidden, batch_index),\n",
    "#                             gap(hidden, batch_index)], dim=1)\n",
    "\n",
    "#         # apply linear layer\n",
    "#         out = self.out(hidden)\n",
    "#         return out\n",
    "    \n",
    "\n",
    "# class EarlyStopping:\n",
    "#     def __init__(self, tolerance=5, min_delta=0):\n",
    "#         self.tolerance = tolerance\n",
    "#         self.min_delta = min_delta\n",
    "#         self.counter = 0\n",
    "#         self.early_stop = False\n",
    "#         self.best_validation_loss = float('inf')\n",
    "\n",
    "#     def __call__(self, train_loss, validation_loss):\n",
    "#         if train_loss > (self.best_validation_loss - self.min_delta):\n",
    "#             self.counter = 0\n",
    "#             self.best_validation_loss = validation_loss\n",
    "\n",
    "#         elif  self.best_validation_loss - self.min_delta > train_loss  :\n",
    "#             self.best_validation_loss = validation_loss\n",
    "#             self.counter += 1\n",
    "#             print(f'case valid better case {self.counter}')\n",
    "#             if self.counter >= self.tolerance:\n",
    "#                 self.early_stop = True\n",
    "\n",
    "#         else : \n",
    "#             print('something wrong')\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# # training function\n",
    "# def train(model,optimizer,loss_fn,epochs:int=1000, is_early_stop:bool = True,show_result_at:int = 0):\n",
    "#     earlystop = EarlyStopping(tolerance=7,min_delta=1)\n",
    "#     train_losses_epoch = []\n",
    "#     valid_losses_epoch = []\n",
    "#     stop_at_epoch = epochs\n",
    "#     for epoch in range(epochs):\n",
    "#         model = model.to(device)\n",
    "#         train_losses = []\n",
    "#         valid_losses = []\n",
    "        \n",
    "#         #train step\n",
    "#         for batch in training_set:\n",
    "#             # Use GPU\n",
    "#             batch.to(device)\n",
    "#             # Reset gradients\n",
    "#             optimizer.zero_grad()\n",
    "#             # Passing the node features and the connection info\n",
    "#             pred  = model(batch.x.float(), batch.edge_index, batch.batch)\n",
    "#             # Calculating the loss and gradients\n",
    "#             train_loss = loss_fn(pred, batch.y)\n",
    "#             train_loss.backward()\n",
    "#             if torch.cuda.is_available() : float_train_loss = float(train_loss.cpu().detach().numpy().astype(float))\n",
    "#             else : float_train_loss = float(train_loss.detach().numpy().astype(float))\n",
    "#             train_losses.append(float_train_loss)\n",
    "#             # Update using the gradients\n",
    "#             optimizer.step()\n",
    "        \n",
    "#         #valid step\n",
    "#         model.eval()\n",
    "#         for batch in validation_set:\n",
    "#             # Use GPU\n",
    "#             batch.to(device)\n",
    "#             # Passing the node features and the connection info\n",
    "#             pred = model(batch.x.float(), batch.edge_index, batch.batch)\n",
    "#             # Calculating the loss\n",
    "#             valid_loss = loss_fn(pred, batch.y)\n",
    "#             if torch.cuda.is_available(): float_valid_loss = float(valid_loss.cpu().detach().numpy().astype(float))\n",
    "#             else : float_valid_loss = float(valid_loss.detach().numpy().astype(float))\n",
    "#             valid_losses.append(float_valid_loss)\n",
    "        \n",
    "#         #calculate average loss\n",
    "#         average_train_loss = sum(train_losses)/len(train_losses)\n",
    "#         average_valid_loss = sum(valid_losses)/len(valid_losses)\n",
    "#         train_losses_epoch.append(average_train_loss)\n",
    "#         valid_losses_epoch.append(average_valid_loss)\n",
    "#         if (show_result_at != 0) and ((epoch+1)%show_result_at == 0 ): print(f\"at epoch : {epoch} train_loss = {average_train_loss}  valid_loss = {average_valid_loss}\")\n",
    "#         if (is_early_stop):earlystop(train_loss=average_train_loss, validation_loss=average_valid_loss)\n",
    "#         if earlystop.early_stop and is_early_stop:\n",
    "#             stop_at_epoch = epoch+1\n",
    "#             print(f'result at {epoch+1} is {earlystop.early_stop}')\n",
    "#             break\n",
    "#     # test_result = test(model,loss_fn=loss_fn)\n",
    "#     return train_losses_epoch, valid_losses_epoch, stop_at_epoch\n",
    "\n",
    "# def test(model,loss_fn):\n",
    "#     test_loss_list = []\n",
    "#     for batch in test_set:\n",
    "#         batch.to(device)\n",
    "#         # Passing the node features and the connection info\n",
    "#         pred = model(batch.x.float(), batch.edge_index, batch.batch)\n",
    "#         # Calculating the loss\n",
    "#         test_loss = loss_fn(pred, batch.y)\n",
    "#         if torch.cuda.is_available(): float_test_loss = float(test_loss.cpu().detach().numpy().astype(float))\n",
    "#         else : float_test_loss = float(test_loss.detach().numpy().astype(float))\n",
    "#         test_loss_list.append(float_test_loss)\n",
    "#     average_train_loss = sum(test_loss_list)/len(test_loss_list)\n",
    "#     return average_train_loss\n",
    "\n",
    "# # def grid_search(model, loss_fn, epoch_list, embed_node_list, embed_classifire_layer):\n",
    "\n"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.9.11"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
